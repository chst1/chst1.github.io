<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="自然语言处理,">










<meta name="description" content="记录一些工作中实际使用到的一些文本处理相关内容。">
<meta name="keywords" content="自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="文本处理">
<meta property="og:url" content="http://yoursite.com/2021/09/30/文本处理相关/index.html">
<meta property="og:site_name" content="chst&#39;s Blog">
<meta property="og:description" content="记录一些工作中实际使用到的一些文本处理相关内容。">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-b9b914067209c1d6b4a9cd8279ac77ff_1440w.jpg">
<meta property="og:updated_time" content="2023-07-16T04:47:35.313Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="文本处理">
<meta name="twitter:description" content="记录一些工作中实际使用到的一些文本处理相关内容。">
<meta name="twitter:image" content="https://pic4.zhimg.com/80/v2-b9b914067209c1d6b4a9cd8279ac77ff_1440w.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/09/30/文本处理相关/">





  <title>文本处理 | chst's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">chst's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">个人网站</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/09/30/文本处理相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chst">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chst's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">文本处理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-30T13:39:35+08:00">
                2021-09-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/09/30/文本处理相关/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2021/09/30/文本处理相关/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  
                </span>
              
            </div>
          

          
              <div class="post-description">
                  记录一些工作中实际使用到的一些文本处理相关内容。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="敏感词过滤"><a href="#敏感词过滤" class="headerlink" title="敏感词过滤"></a>敏感词过滤</h1><p>使用字典树+KMP算法。字典的子节点使用hashmap以加速。在字典中的每个节点中增加KMP中的偏移。为了使得可以匹配出多个字符，可以在字典树的根节点也设置偏移，解决类似如下情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">敏感词：</span><br><span class="line">ABCD</span><br><span class="line">BCDE</span><br><span class="line"></span><br><span class="line">查询串</span><br><span class="line">ABCDEF</span><br></pre></td></tr></table></figure>
<p>在根节点中设置偏移，可以查询出全部出现的两个敏感词。</p>
<p>为了能够处理出现多个词才会命中的情况，例如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">敏感词：</span><br><span class="line">ABC&amp;CD</span><br></pre></td></tr></table></figure>
<p>即出现这两个时才过滤掉。这时，可以在根节点建立一个字典，其中key表示第几个干预词，value表示需要出现的term数量。这是为了解决如下情况的问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ABC&amp;DEF</span><br><span class="line">DEF&amp;XYZ</span><br></pre></td></tr></table></figure>
<p>此时对于DEF字典树的终节点（F）就有应该是<code>{1:2, 2:2}</code>。这里表示，第一个敏感词过滤需要两个term（其自身为其中一个），第二个敏感词过滤也需要两个。</p>
<h1 id="中文切词"><a href="#中文切词" class="headerlink" title="中文切词"></a>中文切词</h1><p>参考jieba做的中文切词，jieba存在两种方式。一个是词典查找，另一个是hmm（隐马尔可夫链）。</p>
<h2 id="生成式与判别式"><a href="#生成式与判别式" class="headerlink" title="生成式与判别式"></a>生成式与判别式</h2><p>在机器学习中任务是从属性X预测标记Y，判别模型求的是P(Y|X)，即后验概率；而生成模型最后求的是P(X,Y)，即联合概率。从本质上来说：</p>
<p><strong>判别模型之所以称为“判别”模型</strong>，是因为其根据X“判别”Y；</p>
<p>而<strong>生成模型之所以称为“生成”模型</strong>，是因为其预测的根据是联合概率P(X,Y)，而联合概率可以理解为“生成”(X,Y)样本的概率分布（或称为 依据）；具体来说，机器学习已知X，从Y的候选集合中选出一个来，可能的样本有$(X,Y_1), (X,Y_2), (X,Y_3),……，(X,Y_n)$,实际数据是如何“生成”的依赖于P(X,Y)，那么最后的预测结果选哪一个Y呢？那就选“生成”概率最大的那个。</p>
<p>对于使用生成式，其实求取的是间接的P(Y|X)。由于</p>
<script type="math/tex; mode=display">
P(X,Y) = P(Y|X) * P(X) = P(X|Y)*P(Y)</script><p>对于P(X)来说，由于X为输入，对于所有的输出Y来说，P(X)是一样的，因此，求P(X,Y)的最大就是求P(Y|X)的最大值。而且在生成式中相对来说，P(X|Y)和P(Y)更容易获取到，所以使用生成式来间接求取P(Y|X)。</p>
<p>假设有四个samples(x为输入，y为输出)： </p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>sample1</th>
<th>sample1</th>
<th>sample1</th>
<th>sample1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>y</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>生成式模型的世界是这个样子：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y=0</th>
<th>y=1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x=0</td>
<td>1/2</td>
<td>0</td>
</tr>
<tr>
<td>x=1</td>
<td>1/4</td>
<td>1/4</td>
</tr>
</tbody>
</table>
</div>
<p>即：</p>
<script type="math/tex; mode=display">
\sum{P(x,y) = 1}</script><p>求取的是联合概率分布。</p>
<p>而判定式模型的世界是这个样子：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>y=0</th>
<th>y=1</th>
</tr>
</thead>
<tbody>
<tr>
<td>x=0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>x=1</td>
<td>1/2</td>
<td>1/2</td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
\sum{P(y|x)} = 1</script><p>即判别式求取的是条件概率分布。</p>
<h3 id="生成式求解"><a href="#生成式求解" class="headerlink" title="生成式求解"></a>生成式求解</h3><p>定义训练集为（C，X），C={c1,c2,…cn}是n个样本的label，X={x1,x2,…xn}是n个训练样本的feature，定义单个测试数据为(c,x)。</p>
<p>训练完成后，输入测试数据，判别式直接给出P(c|x)，即输出（label）关于输入（feature）的条件分布，实际上，这个分布的条件还有训练数据，因为实际上我们是“看过”训练数据之后，学习到了对数据分布的后验认识，然后根据这个认识和测试样本的feature来做出测试样本属于哪个label的决策的，所以有</p>
<script type="math/tex; mode=display">
P(c|x) = P(c|x,C,X)=P(c|x,\theta_{max})</script><p>即依据训练集训练出最大准确率的模型进行使用。</p>
<h3 id="判别式求解"><a href="#判别式求解" class="headerlink" title="判别式求解"></a>判别式求解</h3><p>给定输入x，生成模型可以给出输入和输出的联合分布。生成方法的目标是求出这个联合分布。这里以朴素贝叶斯模型为例，我们要求的目标可以通过：</p>
<script type="math/tex; mode=display">
P(x,c) = P(x|c) * P(c)</script><p>这样将求联合分布的问题转化成了求类别先验概率和类别条件概率的问题，朴素贝叶斯方法做了一个较强的假设————feature的不同维度是独立分布的，简化了类别条件概率的计算。</p>
<h2 id="词典切词方式（Uni-gram）"><a href="#词典切词方式（Uni-gram）" class="headerlink" title="词典切词方式（Uni-gram）"></a>词典切词方式（Uni-gram）</h2><p>首先建立词槽字典，比如如下样式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">北京大学 2053</span><br><span class="line">北京大 0</span><br><span class="line">大学 20025</span><br><span class="line">去 123402</span><br><span class="line">玩 4207</span><br><span class="line">北京 34488</span><br><span class="line">北 17860</span><br><span class="line">京 6583</span><br><span class="line">大 144099</span><br><span class="line">学 17482</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这里，前面是词槽，后面是出现概率。</p>
<p>导入该词典配置，建立一个前缀词词典。这里我觉得建立一个字典树更合适，在词典的最后一个子中标识为终点，并且包含其概率。</p>
<p>之后对待切词的文本进行前缀词查找，即将待匹配的文本的每一个字符作为起始字符，在前缀词典中进行前缀匹配查找，对于字典树来说，需要遍历到所以节点（保证找到所有的切词，而不能是找到一个末尾节点就直接返回）。为了加快速度，字典树中，子节点可以使用hashmap进行存储。这样可以直接进行查找，加快速度。此时对于待匹配的文本，会生成一个有向无环图，例如：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b9b914067209c1d6b4a9cd8279ac77ff_1440w.jpg" alt="img"></p>
<p>对于每一个字符，其存储的是从当前节点到切词末尾节点的边。对于上图的存储大致为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0: [0]</span><br><span class="line">1: [1,2,4]</span><br><span class="line">2: [2]</span><br><span class="line">3: [3,4]</span><br><span class="line">4: [4]</span><br><span class="line">5: [5]</span><br></pre></td></tr></table></figure>
<p>存储末尾位置的同时，也要存储对应的概率，这是为了计算最优路径。</p>
<p>构建了有向无环图后，就可以找到从起始到末尾的多条可选路径。这时，我们可以使用动态规划求解概率最大路径，其中转移函数为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i] = max(dp[k] + p[i][k])</span><br></pre></td></tr></table></figure>
<p>其中k为从节点i出发可以到达的节点k。但由于我们只存储了由前到后的边，未存储从后到前的节点，因此在动态规划时，需要从最后一个节点进行计算。在进行计算最大概率路径同时存储最优路径，当计算到第一个节点时即获取到最优切词结果。</p>
<h2 id="HMM（隐马尔可夫链）"><a href="#HMM（隐马尔可夫链）" class="headerlink" title="HMM（隐马尔可夫链）"></a>HMM（隐马尔可夫链）</h2><p>上述的方法只能处理在词典中出现的词，对于词典中未出现的词其效果很差。因此这里可以使用HMM来处理未出现在词典中的query（出现未出现都可以）。</p>
<p>HMM是生成式。</p>
<p>利用HMM模型进行分词，主要是将分词问题视为一个序列标注（sequence labeling）问题，其中，句子为观测序列，分词结果为状态序列。首先通过语料训练出HMM相关的模型，然后利用Viterbi算法进行求解，最终得到最优的状态序列，然后再根据状态序列，输出分词结果。</p>
<p>序列标注，就是将输入句子和分词结果当作两个序列，<strong>句子为观测序列</strong>，<strong>分词结果为状态序列</strong>，当完成状态序列的标注，也就得到了分词结果。</p>
<p>每个字处在词语中的4种可能状态，B、M、E、S，分别表示Begin（这个字处于词的开始位置）、Middle（这个字处于词的中间位置）、End（这个字处于词的结束位置）、Single（这个字是单字成词）。</p>
<p>使用HMM基于两点假设(s为状态，o为观测)：</p>
<p><strong>齐次马尔科夫性假设</strong>，即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其它时刻的状态及观测无关，也与时刻t无关；</p>
<script type="math/tex; mode=display">
P(s_{i+1}|s_1,s_2,...s_n, o_1, o_2, ...o_n) = P(s_{i+1}|s_i)</script><p><strong>观测独立性假设</strong>，即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其它观测和状态无关，</p>
<script type="math/tex; mode=display">
P(o_i|s_1,s_2,...s_n,o_1, o_2, ... o_n) = P(o_i|s_i)</script><p>对于隐马尔科夫讲解可以看下如下文档：</p>
<p><a href="https://zhuanlan.zhihu.com/p/31926943" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31926943</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/31943292" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31943292</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/31999081" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31999081</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32080000" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32080000</a></p>
<p>最终我们求解的是对于给定的观察序列O(o1,o2,…oT，获取其与状态序列I(s1,s2,…sT)的联合分布概率最大值，即</p>
<script type="math/tex; mode=display">
P(O,I) = P(o_1,o_2,...o_T,s_1,s_2,...s_T)=P(O,I) = P(O|I)*P(I)</script><p>由于</p>
<script type="math/tex; mode=display">
P(AB|C) = P(ABC)/P(C) = \frac{P(ABC)/P(BC)}{P(C)/P(BC)} = P(A|BC)*P(B|C)</script><p>将O递归拆分为AB两部分，第一次划分如下：</p>
<script type="math/tex; mode=display">
P(O|I) = P(o_1,o2,...o_{T-1}, o_T|I)=P(o_1,o_2,...o_{T-1}|o_T,I)*P(o_T|I)</script><p><strong>观测独立性假设</strong>，上式变换为</p>
<script type="math/tex; mode=display">
P(o_1,o_2,...o_{T-1}|o_T,I)*P(o_T|I)=P(o_1,o_2,...o_{T-1}|o_T,I)*P(o_T|s_T)</script><p>之后递归的对上式中前一部分进行拆分</p>
<script type="math/tex; mode=display">
P(o_1,o_2,...o_{T-1}|o_T,I) = p(o_1,o_2,...o_{T-2}|o_{T-1},o_T,I)*P(o_{T-1}|o_T,I)=p(o_1,o_2,...o_{T-2}|o_{T-1},o_T,I)*P(o_{T-1}|s_{T-1})</script><p>通过不断递归拆分，最终得到</p>
<script type="math/tex; mode=display">
P(O|I) = P(o_1|s_1)*P(o_2|s_2)*...*P(o_T|s_T)</script><p>所以</p>
<script type="math/tex; mode=display">
P(O,I) =P(o_1|s_1)*P(o_2|s_2)*...*P(o_T|s_T)*P(I)</script><p>对于P(I)来说</p>
<script type="math/tex; mode=display">
P(I) = P(s_1,s_2,...s_T) = P(s_T|s_1,s_2,...s_{T-1})*P(s_1,s_2,...s_{T-1})</script><p>由于<strong>齐次马尔科夫性假设</strong></p>
<script type="math/tex; mode=display">
P(s_T|s_1,s_2,...s_{T-1})*P(s_1,s_2,...s_{T-1}) = P(s_T|s_{T-1})*P(s1,s2,...s_{T-1})</script><p>之后对上式中后半部分继续执行上述变换，则得到</p>
<script type="math/tex; mode=display">
P(I)  = P(s_1)*P(s_2|s_1)*...*P(s_{T-1}|s_T)</script><p>于是，得到最终的概率函数</p>
<script type="math/tex; mode=display">
P(O,I) =P(o_1|s_1)*P(o_2|s_2)*...*P(o_T|s_T)*P(s_1)*P(s_2|s_1)*...*P(s_{T-1}|s_T)</script><p>所以使用HMM我们需要知道三部分值：</p>
<ol>
<li>P(oi|si)：发射概率。当处于状态si时，观察值（即字符为oi）的概率。</li>
<li>P(Si-1|si): 转移概率。即对于i-1的状态为si-1时，i的状态为si的概率。</li>
<li>P(s1): 初始状态概率。即第一个字符为状态s1的概率。</li>
</ol>
<p>上述3部分值的获取就是对HMM模型的训练，jieba获取方式是拿98年报纸和其他杂志信息中的切词，统计词频，获取对应每一部分的概率。这三部分概率均存储于文件中，对应格式如下。概率值都是取对数之后的结果（可以让概率相乘转变为概率相加），其中-3.14e+100代表负无穷，对应的概率值就是0。</p>
<ol>
<li><p>发射概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">P=&#123;&apos;B&apos;: &#123;&apos;一&apos;: -3.6544978750449433,</span><br><span class="line">&apos;丁&apos;: -8.125041941842026,</span><br><span class="line">&apos;七&apos;: -7.817392401429855,</span><br><span class="line">...</span><br><span class="line">&apos;S&apos;: &#123;&apos;:&apos;: -15.828865681131282,</span><br><span class="line">&apos;一&apos;: -4.92368982120877,</span><br><span class="line">&apos;丁&apos;: -9.024528361347633,</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>P[‘B’][‘一’]代表的含义就是状态处于’B’，而观测的字是‘一’的概率对数值为P[‘B’][‘一’] = -3.6544978750449433</p>
</li>
<li><p>转移概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P=&#123;&apos;B&apos;: &#123;&apos;E&apos;: -0.510825623765990, &apos;M&apos;: -0.916290731874155&#125;,</span><br><span class="line">&apos;E&apos;: &#123;&apos;B&apos;: -0.5897149736854513, &apos;S&apos;: -0.8085250474669937&#125;,</span><br><span class="line">&apos;M&apos;: &#123;&apos;E&apos;: -0.33344856811948514, &apos;M&apos;: -1.2603623820268226&#125;,</span><br><span class="line">&apos;S&apos;: &#123;&apos;B&apos;: -0.7211965654669841, &apos;S&apos;: -0.6658631448798212&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>P[B][E]代表的含义就是从状态B转移到状态E的概率，由P[‘B’][‘E’] = -0.5897149736854513，表示当前状态是B，下一个状态是E的概率对数是-0.5897149736854513，对应的概率值是0.6，相应的，当前状态是B，下一个状态是M的概率是0.4，说明当我们处于一个词的开头时，下一个字是结尾的概率要远高于下一个字是中间字的概率，符合我们的直觉，因为二个字的词比多个字的词更常见。</p>
</li>
<li><p>初始状态概率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P=&#123;&apos;B&apos;: -0.26268660809250016,</span><br><span class="line">&apos;E&apos;: -3.14e+100,</span><br><span class="line">&apos;M&apos;: -3.14e+100,</span><br><span class="line">&apos;S&apos;: -1.4652633398537678&#125;</span><br></pre></td></tr></table></figure>
<p>这个概率表说明一个词中的第一个字属于{B、M、E、S}这四种状态的概率，如下可以看出，E和M的概率都是0，这也和实际相符合：开头的第一个字只可能是每个词的首字（B），或者单字成词（S）。</p>
</li>
</ol>
<p>获得这三部分数据后，通过Viterbi算法（动态规划的一种）。</p>
<p>假设给定<a href="https://zh.wikipedia.org/wiki/隐马尔可夫模型" target="_blank" rel="noopener">隐式马尔可夫模型</a>（HMM）状态空间S，共有k个状态，初始状态 i 的概率为 P(i) ，从状态i 到状态 j 的转移概率为 P[i][j]。 令观察到的输出为 y1,…yT。 产生观察结果的最有可能的状态序列 x1,….xT 由递推关系给出：</p>
<script type="math/tex; mode=display">
V_{1,k} = P(y_1|k)*P(k)</script><script type="math/tex; mode=display">
V_{i,k} = max_{x\in S}(p(y_i|k)*p[x][k]*V_{i-1,k})</script><p>通过上述两式即可进行动态规划，在计算第二个式子是，存储下所选择的x，即选择的前一个节点的状态，即可直接在计算完成后获得最优状态序列。对于分词来说，最后一个子只可能是s或e，因此对于最后一个子，只需要比较其状态为s和e大小即可，选择较大者，即</p>
<script type="math/tex; mode=display">
max(V_{T,E}, V_{T,S})</script><h3 id="前向后向算法评估观测序列概率"><a href="#前向后向算法评估观测序列概率" class="headerlink" title="前向后向算法评估观测序列概率"></a>前向后向算法评估观测序列概率</h3><p>问题：已知HMM模型的参数$\lambda=(A,B,\Pi)$。其中𝐴是隐藏状态转移概率的矩阵，𝐵是观测状态生成概率的矩阵，$\Pi$是隐藏状态的初始概率分布。隐藏状态的状态空间为N。同时我们也已经得到了观测序列$O=\{o_1,o_2,…o_T\}$,现在我们要求观测序列𝑂在模型𝜆下出现的条件概率$P(O|λ)$。</p>
<p>暴力算法为</p>
<script type="math/tex; mode=display">
P(O|\lambda)=\sum_IP(O,I|\lambda)</script><p>列出所有可能的I，求出每一个对应的$P(O,I|\lambda)$。但复杂度过高。对于含有N个隐藏状态的需求来说，其复杂度为$O(TN^T)$。</p>
<h4 id="前向算法求解hmm观测序列概率"><a href="#前向算法求解hmm观测序列概率" class="headerlink" title="前向算法求解hmm观测序列概率"></a>前向算法求解hmm观测序列概率</h4><p>前向算法为动态规划算法，通过定义前向概率来定义动态规划这个局部状态，前向状态定义为：时刻t时隐藏状态为$q_i$,观测状态的序列为$o_1,o_2,…o_t$的概率，即</p>
<script type="math/tex; mode=display">
{\alpha}_t(i) = P(o_1,o_2,...o_t,i_t=q_i|\lambda)</script><p>之后，进行t+1状态的递推。基于时刻t的各个隐藏状态的前向概率，再乘以对应的状态转移概率，即${\alpha}_t(j)a_{ji}$就是时刻t观测到$o_1,o_2,…o_t$,并且时刻t隐藏状态为$q_j$,时刻t+1隐藏状态为$q_i$的概率。即</p>
<script type="math/tex; mode=display">
{\alpha}_t(j)a_{ji}=P(o_1,o_2,...o_t,i_t=q_j|\lambda)P(i_{t+1}=q_i|i_t=q_j,\lambda)\\=P(o_1,o_2,...o_t,i_t=q_j|\lambda)P(i_{t+1}=q_i|o_1,o_2,...o_t,i_t=q_j,\lambda)\\=P(o_1,o_2,...o_t,i_t=q_j,i_{t+1}=q_i|\lambda)</script><p>将所有的t时刻隐藏状态乘以对应的转移概率，并进行求和:</p>
<script type="math/tex; mode=display">
\sum_{j=1}^N{\alpha}_t(j)a_{ji}=\sum_{j=1}^NP(o_1,o_2,...o_t,i_t=q_j,i_{t+1}=q_i|\lambda)=P(o_1,o_2,...o_t,i_{t+1}=q_i|\lambda)</script><p>即得到时刻t观测序列为$o_1,o_2,…o_t$,并且时刻t+1时刻隐藏状态为$q_i$的概率。由于观测状态只依赖于t+1时刻的隐藏状态$q_i$，因此:</p>
<script type="math/tex; mode=display">
[\sum_{j=1}^N{\alpha}_t(j)a_{ji}]b_i(o_{t+1})=P(o_1,o_2,...o_t,i_{t+1}=q_i|\lambda)P(o_{t+1}|i_{t+1}=q_i,\lambda)\\=P(o_1,o_2,...o_t,i_{t+1}=q_i|\lambda)P(o_{t+1}|o_1,o_2,...o_t,i_{t+1}=q_i,\lambda)=P(o_1,o_2,...o_t,o_{t+1},i_{t+1}=q_i|\lambda)</script><p>因此，我们就得到了递推公式，即</p>
<script type="math/tex; mode=display">
{\alpha}_{t+1}(i)=[\sum_{j=1}^N{\alpha}_t(j)a_{ji}]b_i(o_{t+1})</script><p>动态规划从1开始，到时刻T结束，${\alpha}_T(i)$表示在时刻T关系序列为$o_1,o_2,…o_T$，并且时刻T隐藏状态为$q_i$的概率，所以，只要将所以隐藏状态对于的概率相加，即$\sum_{i=1}^N{\alpha}_T(i)$即得到时刻T观测序列为$o_1,o_2,…o_T$的概率。</p>
<p>总结前向算法：</p>
<p>输入：HMM模型$\lambda=(A,B,\Pi)$，观测序列$O=(o_1,o_2,…o_T)$</p>
<p>输出：观测序列概率$P(O|\lambda)$。</p>
<ol>
<li><p>计算时刻1的各个隐藏状态前向概率：</p>
<script type="math/tex; mode=display">
{\alpha}_1(i)={\pi}_ib_i(o_1)=P(i_1=q_i|\lambda)P(o_1|i_1=q_i,\lambda)=P(o_1,i_1=q_i,\lambda),i=1,2,3,...N</script></li>
<li><p>递推时刻2，3，…T时刻的前向概率：</p>
<script type="math/tex; mode=display">
{\alpha}_{t+1}(i)=[\sum_{j=1}^N{\alpha}_t(j)a_{ji}]b_i(o_{t+1}),i=1,2...N</script></li>
<li><p>计算最终结果：</p>
<script type="math/tex; mode=display">
P(O|\lambda)=\sum_{i=1}^N{\alpha}_T(i)</script></li>
</ol>
<p>从递推公式可以看出，其时间复杂度为$O(TN^2)$。</p>
<h4 id="后向算法求解HMM观测序列概率"><a href="#后向算法求解HMM观测序列概率" class="headerlink" title="后向算法求解HMM观测序列概率"></a>后向算法求解HMM观测序列概率</h4><p>后向算法与前向算法类似，也是使用动态规划求解，不过选择的局部状态为后向概率，后向概率为：</p>
<script type="math/tex; mode=display">
\beta_t(i)=P(o_{t+1},o_{t+2},...o_T|i_t=q_i,\lambda)</script><p>递推公式为：</p>
<script type="math/tex; mode=display">
{\beta}_t(i)=\sum_{j=1}^NP(o_{t+1},o_{t+2},...o_T,i_{t+1}=q_j|i_t=q_i,\lambda)\\=\sum_{j=1}^NP(o_{t+1},o_{t+2},...o_T|i_{t+1}=q_j,i_t=q_i,\lambda)P(i_{t+1}|i_t,\lambda)=\sum_{j=1}^NP(o_{t+1},o_{t+2},...o_T|i_{t+1}=q_j,\lambda)P(i_{t+1}|i_t,\lambda)\\=\sum_{j=1}^NP(o_{t+1}|o_{t+2},...o_T,i_{t+1},\lambda)P(o_{t+2},...o_T|i_{t+1},\lambda)P(i_{t+1}=q_j|i_t=q_i)\\=\sum_{j+1}^NP(o_{t+1}|i_{t+1},\lambda)P(o_{t+2},...o_T|i_{t+1},\lambda)P(i_{t+1}=q_j|i_t=q_i)=\sum_{j+1}^Nb_j(o_{t+1}){\beta}_t(i)a_{ij}</script><p>后向算法步骤为：</p>
<p>输入：HMM模型$lambda=(A,B,\Pi)$,观测序列$O=(o_1,o_2,…o_T)$。</p>
<p>输出：观测序列概率$P(O|\lambda)$</p>
<ol>
<li><p>初始化时刻T的各个隐藏状态后向概率：</p>
<script type="math/tex; mode=display">
{\beta}_T(i)=1, i=1,2,...N,i=1,2,...N</script></li>
<li><p>递推时刻T-1,T-2,…1时刻的后向概率：</p>
<script type="math/tex; mode=display">
{\beta}_t(i)=\sum_{j+1}^Nb_j(o_{t+1}){\beta}_t(i)a_{ij},i=1,2,...N</script></li>
<li><p>计算最终结果：</p>
<script type="math/tex; mode=display">
P(O|\lambda)=\sum_{i=1}^N{\pi}_ib_i(o_1){\beta}_1(i)</script></li>
</ol>
<p>其复杂度为$O(TN^2)$。</p>
<h4 id="HMM常用概率的计算"><a href="#HMM常用概率的计算" class="headerlink" title="HMM常用概率的计算"></a>HMM常用概率的计算</h4><ol>
<li><p>给定模型$\lambda$和观测序列$O$，在时刻t处于状态$q_i$的概率记为：</p>
<script type="math/tex; mode=display">
{\gamma}_t(i)=P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)} {p(O|\lambda)}</script><p>由于</p>
<script type="math/tex; mode=display">
P(i_t=q_i,O|\lambda)={\alpha}_t(i){\beta}_t(i)=P(o_1,o_2,...o_t,i_t=q_i|\lambda)P(o_{t+1},o_{t+2},...o_T|i_t=q_i,\lambda)\\=P(o_1,o_2,...o_t|i_t=q_i,\lambda)P(o_{t+1},o_{t+2},...o_T|i_t=q_i,\lambda)P(i_t=q_i|\lambda)=P(O|i_t=q_i,\lambda)P(i_t|\lambda)=P(i_t=q_i,O|\lambda)</script><p>因此：</p>
<script type="math/tex; mode=display">
{\gamma}_t(i)=P(i_t=q_i|O,\lambda)=\frac{ {\alpha}_t(i){\beta}_t(i)}{\sum_{j=1}^N{\alpha}_t(i){\beta}_t(i)}</script></li>
<li><p>给定模型$\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$，且时刻t+1处于状态$q_j$的概率记为:</p>
<script type="math/tex; mode=display">
{\xi}_i(i,j)=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)}</script><p>由于</p>
<script type="math/tex; mode=display">
P(i_t=q_i,i_{t+1}=q_j,O|\lambda)\\=P(i_t=q_i,o_1,...o_t|\lambda)P(i_{t+1}=q_j,o_{t+1},...o_T|i_t=q_i,o_1,...o_t,\lambda)\\={\alpha}_t(i)P(i_{t+1}=q_j,o_{t+1},...o_T|i_t=q_i,\lambda)\\={\alpha}_t(i)P(o_{t+2},...o_T|i_{t+1}=q_j,\lambda)P(i_{t+1}=q_j,o_{t+1}|o_{t+2},...o_T,i_t=q_i,\lambda)\\={\alpha}_t(i){\beta}_{t+1}(j)P(i_{t+1}=q_j,o_{t+1}|i_t=q_i,\lambda)\\={\alpha}_t(i){\beta}_{t+1}(j)P(i_{t+1}=q_j|i_t=q_i,\lambda)P(o_{t+1}|i_{t+1}=q_j,i_t=q_i,\lambda)\\={\alpha}_t(i){\beta}_{t+1}(j)a_{ij}b_j(o_{t+1})</script><p>因此</p>
<script type="math/tex; mode=display">
{\xi}_i(i,j)=\frac{ {\alpha}_t(i){\beta}_{t+1}(j)a_{ij}b_j(o_{t+1})}{\sum_i^N\sum_j^N{\alpha}_t(i){\beta}_{t+1}(j)a_{ij}b_j(o_{t+1})}</script></li>
</ol>
<h2 id="EM（期望极大算法）"><a href="#EM（期望极大算法）" class="headerlink" title="EM（期望极大算法）"></a>EM（期望极大算法）</h2><p>EM算法用于含有隐变量的概率模型的极大似然估计或极大后验概率估计。EM算法分为两步：E步，求期望（expection）；M步，求极大（maximization）。</p>
<p>一般的，将Y表示为观察随机变量的数据，Z表示隐随机变量的数据。Y和Z连在一起称为完全数据。假设Y和Z的联合概率分布为</p>
<script type="math/tex; mode=display">
P(Y,Z|\theta)</script><p>则完全数据的对数似然函数为</p>
<script type="math/tex; mode=display">
logP(Y,Z|\theta)</script><p>EM算法通过迭代求解</p>
<script type="math/tex; mode=display">
L(\theta) = logP(Y|\theta)</script><p>的极大似然估计，来优化参数。</p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>输入：观测变量Y，隐变量数据Z，联合分布$P(Y,Z|\theta)$，条件概率分布$ P(Z|Y,\theta)$;</p>
<p>输出：参数模型$ \theta$；</p>
<p>（1）选择参数初始值$ {\theta}^{(0)}$，开始迭代。</p>
<p>（2）E步：记$ {\theta}^{(i)} $为第i次迭代参数$\theta$的估计值，在第$i+1$次迭代的E步，计算</p>
<script type="math/tex; mode=display">
Q(\theta,{\theta}^{(i)}) = E_Z[logP(Y,Z|\theta)|Y,{\theta}^{(i)}] = \sum_Z{P(Z|Y,{\theta}^{(i)})logP(Y,Z|\theta)}</script><p>其中$P(Z|Y,{\theta}^{(i)})$是在给定观测数据Y和当前的参数估计${\theta}^{(i)}$下隐变量Z的条件概率分布。</p>
<p>（3）M步：求使得$Q(\theta,{\theta}^{(i)})$极大化的$\theta$,确定第i+1次迭代的参数的估计值${\theta}^{(i+1)}$</p>
<script type="math/tex; mode=display">
{\theta}^{(i+1)} = arg{\underset {\theta}{max} }Q(\theta,{\theta}^{(i)})</script><p>（4）重复2，3两步，直到收敛。</p>
<p>Q函数为：完全数据的对数似然函数$logP(Y,Z|\theta)$关于在给定观测数据Y和当前参数${\theta}^{(i)}$下对未观测数据Z的条件概率分布$P(Z|Y,{\theta}^{(i)})$的期望称为$Q$函数，即</p>
<script type="math/tex; mode=display">
Q(\theta,{\theta}^{(i)}) = E_Z[logP(Y,Z|\theta)|Y,{\theta}^{(i)}]</script><p>几点注意：</p>
<ol>
<li><p>EM算法对初值敏感</p>
</li>
<li><p>迭代停止条件，一般是对较小的正数${\epsilon}_1,{\epsilon}_2$,满足</p>
<script type="math/tex; mode=display">
||{\theta}^{i+1} - {\theta}^i||<{\epsilon}_1 或 ||Q(\theta^{i+1},{\theta}^{(i)})-Q(\theta^{i},{\theta}^{(i)})||<{\epsilon}_2</script></li>
</ol>
<h3 id="EM算法的导出（可行性证明）"><a href="#EM算法的导出（可行性证明）" class="headerlink" title="EM算法的导出（可行性证明）"></a>EM算法的导出（可行性证明）</h3><p>对于含有隐变量的概率模型，目标是极大化观测数据（不完全数据）Y关于参数$\theta$的对数似然函数，即：</p>
<script type="math/tex; mode=display">
L(\theta) = logP(Y|\theta) = log\sum_z{P(Y,Z|\theta)} = log(\sum_zP(Y|Z,\theta)P(Z|\theta))</script><p>假设第i次迭代后$\theta$的估计值为${\theta}^i$,希望新的估计值增加，即$L(\theta) &gt; L({\theta}^i)$,并逐步达到最大值，此时，考虑二者差值：</p>
<script type="math/tex; mode=display">
L(\theta) - L({\theta}^i) = log(\sum_ZP(Y|Z,\theta)P(Z|\theta))-logP(Y|{\theta}^i)=log(\sum_ZP(Z|Y,{\theta}^i)\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)})-logP(Y|{\theta}^i)</script><p>利用Jensen不等式</p>
<script type="math/tex; mode=display">
log\sum_j{\lambda}_jy_j >= \sum_j{\lambda}_jlogy_i,其中{\lambda}_j>=0,\sum_j{\lambda}_i=1</script><p>得到</p>
<script type="math/tex; mode=display">
L(\theta)-L({\theta}^i) = log(\sum_ZP(Z|Y,{\theta}^i)\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)})-logP(Y|{\theta}^i)>=\sum_ZP(Z|Y,{\theta}^i)log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)}-logP(Y|{\theta}^i)</script><p>由于</p>
<script type="math/tex; mode=display">
\sum_ZP(Z|Y,{\theta}^i) =1</script><script type="math/tex; mode=display">
logP(Y|{\theta}^i) = logP(Y|{\theta}^i) \sum_ZP(Z|Y,{\theta}^i)=\sum_ZP(Z|Y,{\theta}^i)P(Y|{\theta}^i)</script><p>因此</p>
<script type="math/tex; mode=display">
L(\theta)-L({\theta}^i) >= \sum_ZP(Z|Y,{\theta}^i)log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)P(Y|{\theta}^i)}</script><p>令</p>
<script type="math/tex; mode=display">
B(\theta,{\theta}^i)=L({\theta}^i)+\sum_ZP(Z|Y,{\theta}^i)log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)P(Y|{\theta}^i)}</script><p>则</p>
<script type="math/tex; mode=display">
L(\theta)>=B(\theta,{\theta}^i)</script><p>函数$B(\theta,{\theta}^i)$是$L(\theta)$的下界，并且根据上面的式子可得$L({\theta}^i) = B(\theta,{\theta}^i)$。任何使得B增大的$\theta$，也可以使得$L(\theta)$增大，选择${\theta}^{i+1}$使得B达到最大，即</p>
<script type="math/tex; mode=display">
{\theta}^{i+1}=arg\underset{ {\theta} }{max}B(\theta,{\theta}^i)</script><p>省区对$\theta$的极大化而言是常数的项，则</p>
<script type="math/tex; mode=display">
{\theta}^{i+1}=arg\underset{ {\theta} }{max}(L({\theta}^i)+\sum_ZP(Z|Y,{\theta}^i)log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,{\theta}^i)P(Y|{\theta}^i)})\\=arg\underset{ {\theta} }{max}(\sum_ZP(Z|Y,{\theta}^i)log(P(Y|Z,\theta)P(Z|\theta)) = arg\underset{ {\theta} }{max}(\sum_ZP(Z|Y,{\theta}^i)logP(Y,Z|\theta))=arg\underset{ {\theta} }{max}Q(\theta,{\theta}^i)</script><h3 id="EM算法生成HMM参数（鲍姆-韦尔奇算法）"><a href="#EM算法生成HMM参数（鲍姆-韦尔奇算法）" class="headerlink" title="EM算法生成HMM参数（鲍姆-韦尔奇算法）"></a>EM算法生成HMM参数（鲍姆-韦尔奇算法）</h3><p>训练数据为$\{(O_1,I_1),(O_2,I_2),…(O_D,I_D)\}$,其中任意观测序列$O_d=\{o_1^d,o_2^d,…o_T^d\}$，其对应的未知的隐藏状态序列为$I_d=\{i_1^d,i_2^d,…I_T^d\}$。</p>
<p>算法E步，先计算联合分布$P(O,I|\lambda)$,表达式如下：</p>
<script type="math/tex; mode=display">
P(O,I|\lambda)= \Pi_{d=1}^D{\pi}_{i_1^d}b_{i_1^d}(o_1^d)a_{i_1^d,i_2^d}b_{i_2^d}(o_2^d)...a_{i_{T-1}^d,i_T^d}b_{i_T^d}(o_T^d)</script><p>其中${\pi}_i$为初始状态概率，$b_i(o)$为发射概率，$a_{i_1,i_2}$为转移概率。</p>
<p>E步中我们求取期望为</p>
<script type="math/tex; mode=display">
L(\lambda,\overline{\lambda})=\sum_IP(I|O,\overline{\lambda})logP(O,I|\lambda)</script><p>在M步中，我们要极大化上式，$P(I|O,\overline{\lambda})=\frac{P(I,O|\overline{\lambda})}{P(O|\overline{\lambda})}$,而$P(O|\overline{\lambda})$为常数，因此要极大化的式子等价于</p>
<script type="math/tex; mode=display">
\overline{\lambda} = arg\underset{\lambda}{max}\sum_IP(O,I|\overline{\lambda})logP(O,I|\lambda)=arg\underset{\lambda}{max}\sum_{d=1}^D\sum_IP(O,I|\overline{\lambda})(log{\pi}_{i_1}+\sum_{t=1}^{T-1}loga_{i_t,i_{t+1} }+\sum_{t=1}^Tlogb_{i_t}(o_t))</script><p>隐藏参数$\lambda=(A,B,\Pi)$,上式分别对$A,B,\Pi$求导可得更新的参数模型$\overline{\lambda}$。</p>
<p>首先看模型参数$\Pi$的导数。由于$\Pi$只在第一部分出现，因此</p>
<script type="math/tex; mode=display">
\overline{\pi}=arg\underset{ {\pi}_{i_1} }{max}\sum_{d=1}^D\sum_IP(O,I|\overline{\lambda})log{\pi}_{i_1}=arg\underset{ {\pi}_{i_1} }{max}\sum_{d=1}^D\sum_{i=1}^NP(O,i_1^d|\overline{\lambda})log{\pi}_{i_1}</script><p>由于${\pi}_i$还满足$\sum_{i=1}^N{\pi}_i=1$，根据拉格朗日子乘法，我们得到${\pi}_i$要极大化的拉格朗日函数为</p>
<script type="math/tex; mode=display">
arg\underset{ {\pi}_{i_1} }{max}\sum_{d=1}^D\sum_{i=1}^NP(O,i_1^d|\overline{\lambda})log{\pi}_{i_1}+\gamma(\sum_{i=1}^N{\pi}_i-1)</script><p>其中$\gamma$为拉格朗日系数，上式对${\pi}_1$求偏导，并令结果为0，得到</p>
<script type="math/tex; mode=display">
\sum_{d=1}^DP(O,i_1^d=i|\overline{\lambda})+\gamma{\pi}_i=0</script><p>令i从1到N，从上式可以得到N个式子，对N个式子求和可得</p>
<script type="math/tex; mode=display">
\sum_{d=1}^DP(O|\overline{\lambda})+\gamma=0</script><p>上面两式消去$\gamma$得</p>
<script type="math/tex; mode=display">
{\pi}_i=\frac{\sum_{d=1}^DP(O,i_1^d=i|\lambda)}{\sum_{d=1}^DP(O|\lambda)}=\frac{\sum_{d=1}^DP(O,i_1^d=i|\lambda)}{DP(O|\lambda)}=\frac{\sum_{d=1}^DP(i_1^d=i|O,\lambda)}{D}\\=\frac{\sum_{d=1}^DP(i_1^d=i|O^d,\lambda)}{D}=\frac{\sum_{d=1}^D{\gamma}_1^{(d)}(i)}{D}</script><p>再来看$A$的迭代公式，方法和$\Pi$的类似，由于$A$只在最大化函数里的第二部分出现，因此：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_I\sum_{t=1}^{T-1}P(O,I|\overline{\lambda})loga_{i_t,i_{t+1} }\\=\sum_{d=1}^D\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}P(O,i_t^{d}=i,i_{t+1}^d=j|\overline{\lambda})loga_{ij}</script><p>由于$a_{ij}$满足$\sum_{j=1}^Na_{ij}=1$。因此要最大化的式子为：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}P(O,i_t^{d}=i,i_{t+1}^d=j|\overline{\lambda})loga_{ij}+\sum_{k=1}^N(\eta_k(\sum_{j=1}^Na_{ij}-1))</script><p>对上式的$a_{ij}$求偏导，并使导数为0，得：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{t=1}^{T-1}P(O,i_t^{d}=i,i_{t+1}^d=j|\overline{\lambda})+\sum_{k=1}^N{\eta}_ka_{ij}=0</script><p>与${\pi}_i$类似，对所有N个式子求和可得：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{t=1}^{T-1}P(O,i_t^{d}=i|\overline{\lambda})+\sum_{k=1}^N{\eta}_i=0</script><p>因此：</p>
<script type="math/tex; mode=display">
a_{i,j}=\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}P(O,i_t^{d}=i,i_{t+1}^d=j|\overline{\lambda})}{\sum_{d=1}^D\sum_{t=1}^{T-1}P(O,i_t^{d}=i|\overline{\lambda})}\\=\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i,i_{t+1}^d=j|O,\overline{\lambda})P(O|\overline{\lambda})}{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i|O,\overline{\lambda})P(O|\overline{\lambda})}\\=\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i,i_{t+1}^d=j|O,\overline{\lambda})}{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i|O,\overline{\lambda})}\\\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i,i_{t+1}^d=j|O^d,\overline{\lambda})}{\sum_{d=1}^D\sum_{t=1}^{T-1}P(i_t^{d}=i|O^d,\overline{\lambda})}\\=\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}{\xi}_t^d(i,j)}{\sum_{d=1}^D\sum_{t=1}^{T-1}{\gamma}_t^d(i)}</script><p>最后看B的迭代公式：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_I\sum_{t=1}^TP(O,I|\overline{\lambda})logb_{i_t}(o_t)=\sum_{d=1}^D\sum_{j=1}^N\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})logb_{i_t}(o_t)</script><p>又由于$\sum_{k=1}^Mb_j(o_t=v_k)=1$，因此，与求a时类似：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{j=1}^N\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})logb_{i_t}(o_t)+\sum_{p=1}^N(\eta_p(\sum_{k=1}^Mb_j(o_t=v_k)-1))</script><p>只有在$o_t=v_k$时，$b_j(o_t)$对$b_j(k)$的偏导数才不为0，使用$I(o_t=v_k)$表示，因此对上式求导得：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})I(o_t=v_k)+\sum_{p=1}^N{\eta}_pb_j(o_t=v_k)=0</script><p>与之前类似，对所有M个式子求和，得到：</p>
<script type="math/tex; mode=display">
\sum_{d=1}^D\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})+\sum_{p=1}^N{\eta}_p=0</script><p>因此：</p>
<script type="math/tex; mode=display">
b_j(k)=\frac{\sum_{d=1}^D\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})I(o_t=v_k)}{\sum_{d=1}^D\sum_{t=1}^TP(O,i_t^d=j|\overline{\lambda})} = \frac{\sum_{d=1}^D\sum_{t=1,o_t^d=v_k}^T{\gamma}_t^d(j)}{\sum_{d=1}^D\sum_{t=1}^T{\gamma}_t^d(j)}</script><p>由此，我们得到了模型$\lambda=(A,B,\Pi)$迭代公式。</p>
<p>总结整体流程：</p>
<p>输入：D个观察样本$\{O_1,O_2,…O_D\}$</p>
<p>输出：HMM模型参数</p>
<ol>
<li><p>随机初始化所有${\pi}_i,a_{ij},b_j(k)$。</p>
</li>
<li><p>对每个样本$d=1,2,…D$，用前向后向算法计算${\gamma}_t^d(i),{\xi}_{t}^d(i,j);t=1,2,…T;i=1,2…N;j=1,2,…N$。</p>
</li>
<li><p>更新模型参数：</p>
<script type="math/tex; mode=display">
{\pi}_i =\frac{\sum_{d=1}^D{\gamma}_1^{(d)}(i)}{D} \\
a_{ij}=\frac{\sum_{d=1}^D\sum_{t=1}^{T-1}{\xi}_t^d(i,j)}{\sum_{d=1}^D\sum_{t=1}^{T-1}{\gamma}_t^d(i)}\\
b_j(k)=\frac{\sum_{d=1}^D\sum_{t=1,o_t^d=v_k}^T{\gamma}_t^d(j)}{\sum_{d=1}^D\sum_{t=1}^T{\gamma}_t^d(j)}</script></li>
<li><p>如果参数值已收敛，则停止迭代，输出模型，否则重复2，3步。</p>
</li>
</ol>
<h2 id="CRF-条件随机场"><a href="#CRF-条件随机场" class="headerlink" title="CRF(条件随机场)"></a>CRF(条件随机场)</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/自然语言处理/" rel="tag"># 自然语言处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/27/操作系统设计与实现/" rel="next" title="操作系统设计与实现">
                <i class="fa fa-chevron-left"></i> 操作系统设计与实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/09/30/ES学习/" rel="prev" title="ElasticSearch学习">
                ElasticSearch学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chst</p>
              <p class="site-description motion-element" itemprop="description">人生苦酒,自酿自品</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#敏感词过滤"><span class="nav-number">1.</span> <span class="nav-text">敏感词过滤</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#中文切词"><span class="nav-number">2.</span> <span class="nav-text">中文切词</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#生成式与判别式"><span class="nav-number">2.1.</span> <span class="nav-text">生成式与判别式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生成式求解"><span class="nav-number">2.1.1.</span> <span class="nav-text">生成式求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#判别式求解"><span class="nav-number">2.1.2.</span> <span class="nav-text">判别式求解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词典切词方式（Uni-gram）"><span class="nav-number">2.2.</span> <span class="nav-text">词典切词方式（Uni-gram）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HMM（隐马尔可夫链）"><span class="nav-number">2.3.</span> <span class="nav-text">HMM（隐马尔可夫链）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向后向算法评估观测序列概率"><span class="nav-number">2.3.1.</span> <span class="nav-text">前向后向算法评估观测序列概率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向算法求解hmm观测序列概率"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">前向算法求解hmm观测序列概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#后向算法求解HMM观测序列概率"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">后向算法求解HMM观测序列概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HMM常用概率的计算"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">HMM常用概率的计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EM（期望极大算法）"><span class="nav-number">2.4.</span> <span class="nav-text">EM（期望极大算法）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法"><span class="nav-number">2.4.1.</span> <span class="nav-text">EM算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法的导出（可行性证明）"><span class="nav-number">2.4.2.</span> <span class="nav-text">EM算法的导出（可行性证明）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法生成HMM参数（鲍姆-韦尔奇算法）"><span class="nav-number">2.4.3.</span> <span class="nav-text">EM算法生成HMM参数（鲍姆-韦尔奇算法）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRF-条件随机场"><span class="nav-number">2.5.</span> <span class="nav-text">CRF(条件随机场)</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chst</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="/js/src/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '0j9TGrGA2Aq8e4SO1sUkgQCv-gzGzoHsz',
        appKey: 'Q6jotQjlp43pwpkFCJhQ9s95',
        placeholder: '请留下联系方式，我会尽快回复',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
